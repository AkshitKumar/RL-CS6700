\relax 
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Goal}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Implementation Details}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}True Expected Payoff of Arms}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Initialization of Estimates of Expectation of Arms}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.3}Sampling of Rewards for Arms}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.4}Update of the Estimate of Arms}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Action-Value Algorithms}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}$\epsilon $-Greedy}{3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces $\epsilon $-greedy Algorithm}}{3}}
\newlabel{EGAlgorithm}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Sampling from softmax distribution}{3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Sampling from Softmax Distribution}}{3}}
\newlabel{SoftmaxAlgorithm}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Upper Confidence Bound}{4}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Upper Confidence Bound}}{4}}
\newlabel{UCBAlgorithm}{{3}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experimental Results}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Plots for $\epsilon $-greedy Action Selection for 10 arms}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Plot of average reward of $\epsilon $-greedy action-value methods on the 10-armed testbed. These data are averages over 2000 runs with different bandit problems.}}{5}}
\newlabel{fig:eg1}{{1}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Plot of \% of optimal action selection of $\epsilon $-greedy action-value methods on the 10-armed testbed. These data are averages over 2000 runs with different bandit problems.}}{5}}
\newlabel{fig:eg1}{{2}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Plots for Softmax Action Selection for 10 arms}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Plot of average reward of softmax action selection methods on the 10-armed testbed. These data are averages over 2000 runs with different bandit problems.}}{6}}
\newlabel{fig:eg1}{{3}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Plot of average reward of softmax action selection methods on the 10-armed testbed. These data are averages over 2000 runs with different bandit problems.}}{6}}
\newlabel{fig:eg1}{{4}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Plots for UCB vs $\epsilon $-greedy vs Softmax Action Selection for 10 arms}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Plot comparing the average reward accrued using UCB, $\epsilon $-greedy and Softmax Action Selection on the 10-armed testbed over 1000 timesteps. These data are averaged over 2000 runs of different bandit problems. For $\epsilon $-greedy approach, $\epsilon = 0.1 $ and for softmax action selection, $\tau = 0.1$ where $\tau $ is the temperature parameter.}}{7}}
\newlabel{fig:eg1}{{5}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Plot comparing the \% optimal action selection using UCB, $\epsilon $-greedy and Softmax Action Selection on the 10-armed testbed over 1000 timesteps. These data are averaged over 2000 runs of different bandit problems. For $\epsilon $-greedy approach, $\epsilon = 0.1 $ and for softmax action selection, $\tau = 0.1$ where $\tau $ is the temperature parameter.}}{7}}
\newlabel{fig:eg1}{{6}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Plots for UCB vs $\epsilon $-greedy vs Softmax Action Selection for 1000-arms}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Plots for 1000 time-steps}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Plot comparing the average reward accrued using UCB, $\epsilon $-greedy and Softmax Action Selection on the 1000-armed testbed over 1000 timesteps. These data are averaged over 2000 runs of different bandit problems. For $\epsilon $-greedy approach, $\epsilon = 0.1 $ and for softmax action selection, $\tau = 0.1$ where $\tau $ is the temperature parameter.}}{8}}
\newlabel{fig:eg1}{{7}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Plot comparing the average reward accrued using UCB, $\epsilon $-greedy and Softmax Action Selection on the 1000-armed testbed over 1000 timesteps. These data are averaged over 2000 runs of different bandit problems. For $\epsilon $-greedy approach, $\epsilon = 0.1 $ and for softmax action selection, $\tau = 0.1$ where $\tau $ is the temperature parameter.}}{9}}
\newlabel{fig:eg1}{{8}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Plots for 10000 time-steps}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Plot comparing the average reward accrued using UCB, $\epsilon $-greedy and Softmax Action Selection on the 1000-armed testbed over 10000 timesteps. These data are averaged over 2000 runs of different bandit problems. For $\epsilon $-greedy approach, $\epsilon = 0.1 $ and for softmax action selection, $\tau = 0.1$ where $\tau $ is the temperature parameter.}}{10}}
\newlabel{fig:eg1}{{9}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Plot comparing the average reward accrued using UCB, $\epsilon $-greedy and Softmax Action Selection on the 1000-armed testbed over 10000 timesteps. These data are averaged over 2000 runs of different bandit problems. For $\epsilon $-greedy approach, $\epsilon = 0.1 $ and for softmax action selection, $\tau = 0.1$ where $\tau $ is the temperature parameter.}}{10}}
\newlabel{fig:eg1}{{10}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}Plots for 20000 time-steps}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Plot comparing the average reward accrued using UCB, $\epsilon $-greedy and Softmax Action Selection on the 1000-armed testbed over 20000 timesteps. These data are averaged over 2000 runs of different bandit problems. For $\epsilon $-greedy approach, $\epsilon = 0.1 $ and for softmax action selection, $\tau = 0.1$ where $\tau $ is the temperature parameter.}}{11}}
\newlabel{fig:eg1}{{11}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Plot comparing the average reward accrued using UCB, $\epsilon $-greedy and Softmax Action Selection on the 1000-armed testbed over 20000 timesteps. These data are averaged over 2000 runs of different bandit problems. For $\epsilon $-greedy approach, $\epsilon = 0.1 $ and for softmax action selection, $\tau = 0.1$ where $\tau $ is the temperature parameter.}}{11}}
\newlabel{fig:eg1}{{12}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Analysis of the Plots}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Plots for $\epsilon $-greedy action selection for 10 arms}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Plots for Softmax Action Selection for 10 arms}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Plots for UCB vs $\epsilon $-greedy vs Softmax Action Selection for 10 arms}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Plots for UCB vs $\epsilon $-greedy vs Softmax Action Selection for 1000 arms}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Plots for 1000 timesteps}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Plots for 10000 timesteps}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}Plots for 20000 timesteps}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Code Listing}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Bandit Class for object instantiation}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Python Library for Simulating the Bandit Problem}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Python Script to Generate Plots for $\epsilon $-greedy action selection}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Python Script to Generate Plots for softmax action selection}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Python Script to Generate Plots for comparing UCB vs $\epsilon $-greedy action selection vs softmax action selection}{17}}
